{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\user\\\\OneDrive\\\\문서\\\\Python\\\\Research_ai\\\\Bertopic 1_paper_240921\\\\250202_version5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 출력 설정 (컬럼의 모든 내용이 보이도록 설정)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 완료 : 특수문자 제거, 논문 개행문자(\\r\\n ...) 제거, 논문 초록 시작과 끝의 (<p>....<p>) 제거, NaN 행(4개) 제거 등\n",
    "# 분석 결과 값(bert_cased_similar 컬럼)이 포함된 파일입니다 교수님!\n",
    "pat_pub_similar_all = pd.read_csv('c:\\\\Users\\\\user\\\\OneDrive\\\\문서\\\\Python\\\\Research_ai\\\\Bertopic 1_paper_240921\\\\250202_version5\\\\pat_pub_similar_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patent_count': 14293, 'article_count': 45296, 'full length': 76960}\n"
     ]
    }
   ],
   "source": [
    "print({\n",
    "    'patent_count' : pat_pub_similar_all['patent_id'].nunique(),\n",
    "    'article_count' : pat_pub_similar_all['pubid'].nunique(),\n",
    "    'full length' : len(pat_pub_similar_all)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>pat_abs</th>\n",
       "      <th>patyear</th>\n",
       "      <th>pubid</th>\n",
       "      <th>pub_abs</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>period</th>\n",
       "      <th>bert_cased_similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7747044</td>\n",
       "      <td>A Bayesian belief networkbased architecture fo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2170002.0</td>\n",
       "      <td>Integration of various fingerprint matching al...</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.778748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7277306</td>\n",
       "      <td>A CAM unit has a memory array for storing stor...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>54718459.0</td>\n",
       "      <td>Placement of errorcorrectingcode ECC systems o...</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.859097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8065625</td>\n",
       "      <td>A GUI evaluation system includes an expression...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>10234.0</td>\n",
       "      <td>Designing user interfaces with consistent visu...</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>852158.0</td>\n",
       "      <td>The tutorial starts with an overview of the co...</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1446523.0</td>\n",
       "      <td>A plane separating two point sets in ndimensio...</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.761514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patent_id                                            pat_abs  patyear  \\\n",
       "0    7747044  A Bayesian belief networkbased architecture fo...   2010.0   \n",
       "1    7277306  A CAM unit has a memory array for storing stor...   2007.0   \n",
       "2    8065625  A GUI evaluation system includes an expression...   2011.0   \n",
       "3    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "4    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "\n",
       "        pubid                                            pub_abs  pubyear  \\\n",
       "0   2170002.0  Integration of various fingerprint matching al...   1999.0   \n",
       "1  54718459.0  Placement of errorcorrectingcode ECC systems o...   1991.0   \n",
       "2     10234.0  Designing user interfaces with consistent visu...   1997.0   \n",
       "3    852158.0  The tutorial starts with an overview of the co...   1998.0   \n",
       "4   1446523.0  A plane separating two point sets in ndimensio...   1999.0   \n",
       "\n",
       "   period  bert_cased_similar  \n",
       "0     1.0            0.778748  \n",
       "1     1.0            0.859097  \n",
       "2     1.0            0.718306  \n",
       "3     1.0            0.833586  \n",
       "4     1.0            0.761514  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_pub_similar_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT(cased) tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 문단을 문장 단위로 분리하는 함수\n",
    "def split_into_sentences(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "# 문장 리스트를 생성하는 함수\n",
    "def format_sentences_as_list(paragraph):\n",
    "    sentences = split_into_sentences(paragraph)\n",
    "    return [[sentence] for sentence in sentences]  # 각 문장을 ['sentence.'] 형식으로 변환\n",
    "\n",
    "# 문장 임베딩 계산 함수\n",
    "def get_embedding(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  #[:, 0, :]  # CLS 토큰 임베딩 ▶▶▶ 평균, dim=1로 수정\n",
    "    return embeddings\n",
    "\n",
    "# 두 문단 간 유사도 계산 함수\n",
    "def compute_paragraph_similarity(pat_abs, pub_abs, tokenizer, model):\n",
    "    # 문단을 문장 단위로 나누고 리스트로 변환\n",
    "    pat_sentences = format_sentences_as_list(pat_abs)\n",
    "    pub_sentences = format_sentences_as_list(pub_abs)\n",
    "    \n",
    "    # 문장 임베딩 계산\n",
    "    pat_embeddings = [get_embedding(sent, tokenizer, model) for sent in pat_sentences]\n",
    "    pub_embeddings = [get_embedding(sent, tokenizer, model) for sent in pub_sentences]\n",
    "    \n",
    "    # 문장 쌍 간 유사도 계산\n",
    "    similarities = []\n",
    "    for pat_emb in pat_embeddings:\n",
    "        for pub_emb in pub_embeddings:\n",
    "            sim = cosine_similarity(pat_emb.numpy(), pub_emb.numpy())[0][0]\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    # 평균 유사도 반환\n",
    "    return sum(similarities) / len(similarities)\n",
    "\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 유사도 계산\n",
    "def calculate_similarities(dataframe, tokenizer, model):\n",
    "    similarities = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        pat_abs = row[\"pat_abs\"]\n",
    "        pub_abs = row[\"pub_abs\"]\n",
    "        similarity = compute_paragraph_similarity(pat_abs, pub_abs, tokenizer, model)\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # BERT 모델 및 토크나이저 로드\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "    # 유사도 계산\n",
    "    pat_pub_similar_all[\"bert_cased_similar\"] = calculate_similarities(pat_pub_similar_all, tokenizer, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
