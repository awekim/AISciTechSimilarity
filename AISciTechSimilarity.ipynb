{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Science-Technology Similarity Research\n",
    "- Made by: Sukhee Lee (Ph.D. Candidate) & Keungoui Kim (Ph.D.)\n",
    "- Goal: Measuring Science-Technology Similarity\n",
    "- Data set: WoS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Import & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"H:/GD_awekimm/[YU]/[Research]/02_이석희/01_AI_scitech_similarity/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "from transformers import LongformerTokenizer, LongformerModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_pub_similar_all = pd.read_csv(dir+'pat_pub_similar_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patent_count': 14293, 'article_count': 45296, 'full length': 76960}\n"
     ]
    }
   ],
   "source": [
    "print({\n",
    "    'patent_count' : pat_pub_similar_all['patent_id'].nunique(),\n",
    "    'article_count' : pat_pub_similar_all['pubid'].nunique(),\n",
    "    'full length' : len(pat_pub_similar_all)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patent_id</th>\n",
       "      <th>pat_abs</th>\n",
       "      <th>patyear</th>\n",
       "      <th>pubid</th>\n",
       "      <th>pub_abs</th>\n",
       "      <th>pubyear</th>\n",
       "      <th>period</th>\n",
       "      <th>bert_cased_similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7747044</td>\n",
       "      <td>A Bayesian belief networkbased architecture fo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2170002.0</td>\n",
       "      <td>Integration of various fingerprint matching al...</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.778748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7277306</td>\n",
       "      <td>A CAM unit has a memory array for storing stor...</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>54718459.0</td>\n",
       "      <td>Placement of errorcorrectingcode ECC systems o...</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.859097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8065625</td>\n",
       "      <td>A GUI evaluation system includes an expression...</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>10234.0</td>\n",
       "      <td>Designing user interfaces with consistent visu...</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.718306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>852158.0</td>\n",
       "      <td>The tutorial starts with an overview of the co...</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1446523.0</td>\n",
       "      <td>A plane separating two point sets in ndimensio...</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.761514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>1881981.0</td>\n",
       "      <td>Successive overrelaxation SOR for symmetric li...</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.865539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>3948153.0</td>\n",
       "      <td>Smoothing methods extensively used for solving...</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>4212244.0</td>\n",
       "      <td>Classification of human tumors according to th...</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.841520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>55094576.0</td>\n",
       "      <td>We discuss progress in the development of auto...</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.859736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7395253</td>\n",
       "      <td>A Lagrangian support vector machine solves pro...</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>58224689.0</td>\n",
       "      <td>We consider the unconstrained minimization of ...</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patent_id                                            pat_abs  patyear  \\\n",
       "0    7747044  A Bayesian belief networkbased architecture fo...   2010.0   \n",
       "1    7277306  A CAM unit has a memory array for storing stor...   2007.0   \n",
       "2    8065625  A GUI evaluation system includes an expression...   2011.0   \n",
       "3    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "4    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "5    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "6    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "7    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "8    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "9    7395253  A Lagrangian support vector machine solves pro...   2008.0   \n",
       "\n",
       "        pubid                                            pub_abs  pubyear  \\\n",
       "0   2170002.0  Integration of various fingerprint matching al...   1999.0   \n",
       "1  54718459.0  Placement of errorcorrectingcode ECC systems o...   1991.0   \n",
       "2     10234.0  Designing user interfaces with consistent visu...   1997.0   \n",
       "3    852158.0  The tutorial starts with an overview of the co...   1998.0   \n",
       "4   1446523.0  A plane separating two point sets in ndimensio...   1999.0   \n",
       "5   1881981.0  Successive overrelaxation SOR for symmetric li...   1999.0   \n",
       "6   3948153.0  Smoothing methods extensively used for solving...   2001.0   \n",
       "7   4212244.0  Classification of human tumors according to th...   2001.0   \n",
       "8  55094576.0  We discuss progress in the development of auto...   1992.0   \n",
       "9  58224689.0  We consider the unconstrained minimization of ...   1995.0   \n",
       "\n",
       "   period  bert_cased_similar  \n",
       "0     1.0            0.778748  \n",
       "1     1.0            0.859097  \n",
       "2     1.0            0.718306  \n",
       "3     1.0            0.833586  \n",
       "4     1.0            0.761514  \n",
       "5     1.0            0.865539  \n",
       "6     1.0            0.783934  \n",
       "7     1.0            0.841520  \n",
       "8     1.0            0.859736  \n",
       "9     1.0            0.855521  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_pub_similar_all.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring Similarity with Cross Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Cross-Encoder Model\n",
    "model = CrossEncoder('cross-encoder/stsb-roberta-base') # cross-encoder/ms-marco-MiniLM-L-12-v2\n",
    "\n",
    "similarity_score_list = []\n",
    "for i in range(pat_pub_similar_all.shape[0]):\n",
    "    pair = [(pat_pub_similar_all['pat_abs'][i], pat_pub_similar_all['pub_abs'][i])]\n",
    "    similarity_score = model.predict(pair)[0]\n",
    "    similarity_score_list.append(similarity_score)\n",
    "    \n",
    "pat_pub_similar_all['similarity_score'] = similarity_score_list\n",
    "pat_pub_similar_all.to_csv(dir+'pat_pub_similar_all_250210.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (OLD) Measuring Similarity with Cosine-Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT(cased) tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained('bert-base-cased')d\n",
    "\n",
    "# spaCy 모델 로드\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 문단을 문장 단위로 분리하는 함수\n",
    "def split_into_sentences(paragraph):\n",
    "    doc = nlp(paragraph)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "# 문장 리스트를 생성하는 함수\n",
    "def format_sentences_as_list(paragraph):\n",
    "    sentences = split_into_sentences(paragraph)\n",
    "    return [[sentence] for sentence in sentences]  # 각 문장을 ['sentence.'] 형식으로 변환\n",
    "\n",
    "# 문장 임베딩 계산 함수\n",
    "def get_embedding(texts, tokenizer, model):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  #[:, 0, :]  # CLS 토큰 임베딩 ▶▶▶ 평균, dim=1로 수정\n",
    "    return embeddings\n",
    "\n",
    "# 두 문단 간 유사도 계산 함수\n",
    "def compute_paragraph_similarity(pat_abs, pub_abs, tokenizer, model):\n",
    "    # 문단을 문장 단위로 나누고 리스트로 변환\n",
    "    pat_sentences = format_sentences_as_list(pat_abs)\n",
    "    pub_sentences = format_sentences_as_list(pub_abs)\n",
    "    \n",
    "    # 문장 임베딩 계산\n",
    "    pat_embeddings = [get_embedding(sent, tokenizer, model) for sent in pat_sentences]\n",
    "    pub_embeddings = [get_embedding(sent, tokenizer, model) for sent in pub_sentences]\n",
    "    \n",
    "    # 문장 쌍 간 유사도 계산\n",
    "    similarities = []\n",
    "    for pat_emb in pat_embeddings:\n",
    "        for pub_emb in pub_embeddings:\n",
    "            sim = cosine_similarity(pat_emb.numpy(), pub_emb.numpy())[0][0]\n",
    "            similarities.append(sim)\n",
    "    \n",
    "    # 평균 유사도 반환\n",
    "    return sum(similarities) / len(similarities)\n",
    "\n",
    "\n",
    "# 데이터프레임의 각 행에 대해 유사도 계산\n",
    "def calculate_similarities(dataframe, tokenizer, model):\n",
    "    similarities = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        pat_abs = row[\"pat_abs\"]\n",
    "        pub_abs = row[\"pub_abs\"]\n",
    "        similarity = compute_paragraph_similarity(pat_abs, pub_abs, tokenizer, model)\n",
    "        similarities.append(similarity)\n",
    "    return similarities\n",
    "\n",
    "# 메인 실행 부분\n",
    "if __name__ == \"__main__\":\n",
    "    # BERT 모델 및 토크나이저 로드\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "    # 유사도 계산\n",
    "    pat_pub_similar_all[\"bert_cased_similar\"] = calculate_similarities(pat_pub_similar_all, tokenizer, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
